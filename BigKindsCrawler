from selenium import webdriver
from bs4 import BeautifulSoup
import time
import requests
import re
import csv
import datetime

class Big_Kinds_Crawler(object):
    def __init__(self):
        self.news_press = [] # 언론사
        self.news_headline = [] # 뉴스 헤드라인
        self.news_url = []  # 뉴스 url
        self.news_main_text = []    # 뉴스 본문
        self.news_date = []  # 뉴스 날짜

    @staticmethod
    def preprocess(text):
        text = re.sub('<.+?>|&nbsp;|br|▲|◆|▶|■|○|△|□', '', str(text))    # <> 안 내용, &nbsp;(개행) 제거
        text = text.replace("\n", "")
        text = text.strip()
        return text

    def delete(self, url_count):
        del self.news_press[url_count]
        del self.news_headline[url_count]
        del self.news_url[url_count]
        del self.news_date[url_count]

    def crawl_news_url(self, date_start, date_end):   # 빅카인즈에서 뉴스별 url, 언론사, 헤드라인 크롤

        driver = webdriver.Chrome('/Users/manda/OneDrive/바탕 화면/Utilities/chromedriver_win32/chromedriver')
        driver.implicitly_wait(3)
        driver.get('https://www.bigkinds.or.kr/')

        driver.implicitly_wait(1)

        # 팝업 창 닫기
        html_popup = driver.page_source
        soup_popup = BeautifulSoup(html_popup, 'html.parser')
        if soup_popup.select('#contents > div.popup-container') != None:
            driver.find_element_by_css_selector('div.popup-footer > div > div > button').click()
            
        # 기간 설정
        driver.find_element_by_id('date-filter-btn').click()    # 기간 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(1)').click() # 1일 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(2)').click() # 1주 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(3)').click() # 1개월 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(4)').click() # 3개월 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(5)').click() # 6개월 버튼
        #driver.find_element_by_css_selector('#date-filter-div > div > div:nth-child(1) > button:nth-child(6)').click() # 1년 버튼
        driver.find_element_by_id('search-begin-date').send_keys('\b\b\b\b\b\b\b\b\b\b' + date_start[0:4] + '-' + date_start[4:6] + '-' + date_start[6:]) # 시작 날짜 입력
        driver.find_element_by_id('search-end-date').send_keys('\b\b\b\b\b\b\b\b\b\b' + date_end[0:4] + '-' + date_end[4:6] + '-' + date_end[6:])   # 끝 날짜 입력
        driver.find_element_by_id('date-confirm-btn').click()   # 기간 적용 버튼

        # 언론사 설정
        driver.find_element_by_id('provider-filter-btn').click()    # 언론사 버튼  
        #driver.find_element_by_id('중앙지').click()    # 중앙지 체크박스
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(1) > div > button:nth-child(1)').click()    # 경향
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(2)').click()    # 국민
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(3)').click()    # 내일
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(4)').click()    # 동아
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(5)').click()    # 문화
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(6)').click()    # 서울
        driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(7)').click()    # 세계
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(8)').click()    # 조선
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(9)').click()    # 중앙
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(10)').click()   # 한겨레
        #driver.find_element_by_css_selector('#providers-wrap > div:nth-child(3) > div:nth-child(2) > div > button:nth-child(11)').click()   # 한국

        driver.implicitly_wait(1)

        # 카테고리 설정
        driver.find_element_by_id('category-filter-btn').click()    # 카테고리 버튼
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(1) > div > span:nth-child(3)').click() # 정치
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(2) > div > span:nth-child(3)').click() # 경제
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(3) > div > span:nth-child(3)').click() # 사회
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(4) > div > span:nth-child(3)').click() # 문화
        driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(5) > div > span:nth-child(3)').click() # 국제
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(6) > div > span:nth-child(3)').click() # 지역
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(7) > div > span:nth-child(3)').click() # 스포츠
        #driver.find_element_by_css_selector('#category-tree-wrap > ul > li:nth-child(8) > div > span:nth-child(3)').click() # IT_과학

        driver.find_element_by_css_selector('#news-search-form > div > div > div > div.input-group.main-search__form > span > button').click() # 검색 버튼

        driver.implicitly_wait(3)

        driver.find_element_by_css_selector('#filter-tm-use').click()   #   인사, 부고, 동정, 포토 제외

        time.sleep(1)

        driver.find_element_by_css_selector('#select1 > option:nth-child(3)').click()   # 과거순

        time.sleep(1)

        driver.find_element_by_css_selector('#select2 > option:nth-child(4)').click()   # 100건씩 보기

        time. sleep(1)

        html = driver.page_source
        soup = BeautifulSoup(html, 'html.parser')

        total_count = soup.select_one('#total-news-cnt').get_text().replace(',', '') # 날짜 범위 내 기사 개수
        page_count = (int(total_count) // 100)  # 기사 개수 // 100건씩 보기 = 페이지
        if int(total_count) % 100 != 0:
            page_count += 1
        
        for page in range(1, page_count + 1):   # 1~마지막 페이지
            # 페이지 처리
            
            page_click = page % 7   # 페이지 버튼 7개
            if page_click != 1:
                if page_click == 0:
                    page_click = 7
                page_click += 2
                driver.find_element_by_css_selector('#news-results-pagination > ul > li:nth-child(' + str(page_click) + ') > a').click()    # 페이지 클릭
            elif (page_click == 1) and (page > 1):
                driver.find_element_by_css_selector('#news-results-pagination > ul > li:nth-child(10) > a').click()  # 다음 목록으로 넘어가기 클릭
                '''if page != page_count:
                    time.sleep(2)
                    driver.find_element_by_css_selector('#news-results-pagination > ul > li:nth-child(3) > a').click()'''

            time.sleep(2)

            url = [] # 페이지 별 기사들의 url
            press = []  # 페이지 별 기사들의 언론사
            headline = []   #페이지 별 기사들의 헤드라인
        
            html = driver.page_source
            soup = BeautifulSoup(html, 'html.parser')

            url = soup.select('#news-results > div > div > div > a')  # 기사 url 긁기
            press = soup.select('#news-results > div > div > div > a') # 언론사 긁기
            headline = soup.select('#news-results > div > div.news-item__body > h4') # 기사 제목 긁기
            date = soup.select('#news-results > div > div.news-item__body > div.news-item__meta > span.news-item__date')
            
            for w in range(0, len(url)):
                self.news_url.append(url[w].get('href'))
            for x in range(0, len(press)):
                self.news_press.append(press[x].text)
            for y in range(0, len(date)):
                if 'Invalid' in date[y].text.strip():
                    continue
                self.news_headline.append((headline[y].text).replace('>', '').strip())
                self.news_date.append(date[y].text.strip())

        driver.close()

    def crawl_news_text(self):  # 긁어온 url들 들어가서 뉴스 본문 긁기
        fail_count = 0  # url에 본문 내용이 없을 때 count
        for url_count  in range(0, len(self.news_url)): # 빅카인즈에서 긁어온 url 수만큼 반
            url_html = requests.get(self.news_url[url_count - fail_count]).content
            soup = BeautifulSoup(url_html, 'html.parser')
            print(self.news_url[url_count - fail_count])
            # 각 기사의 언론사에 맞게 크롤
            #if '경향신문' in self.news_press[url_count]:
            #    self.news_main_text.append(self.preprocess(soup.select_one('#articleBody').get_text()))
            if '국민일보' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('#articleBody')
                if tag == None:    # url에 본문 내용 없을 때
                    self.delete(url_count - fail_count) # 모든 list에서 해당 url에 대한 정보 삭제
                    fail_count += 1
                else:
                    self.news_main_text.append(self.preprocess(tag.text))
            elif '내일신문' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('#contents > p')
                if tag == None:
                    self.delete(url_count - fail_count)
                    fail_count += 1
                else:
                    self.news_main_text.append(self.preprocess(tag.text))
            elif '동아일보' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('#content > div > div.article_txt')
                if tag == None:
                    self.delete(url_count - fail_count)
                    fail_count += 1
                else:
                    for span in tag.select('span'):
                        span.decompose()
                    for ul in tag.select('ul'):
                        ul.decompose()
                    for strong in tag.select('strong'):
                        strong.decompose()
                    for a in tag.select('a'):
                        a.decompose()
                    self.news_main_text.append(self.preprocess(tag.text))
            elif '문화일보' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('#NewsAdContent')
                if  tag == None:
                    self.delete(url_count - fail_count)
                    fail_count += 1
                else:
                    self.news_main_text.append(self.preprocess(tag.text))
            elif '서울신문' in self.news_press[url_count - fail_count]:
                if 'go' in self.news_url[url_count - fail_count]:
                    tag = soup.select_one('#article_content')
                    if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                    else:
                        self.news_main_text.append(self.preprocess(tag.text))
                elif 'now' in self.news_url[url_count - fail_count]:
                    tag = soup.select_one('#articleContent')
                    if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                    else:
                        self.news_main_text.append(self.preprocess(tag.text))
                elif 'stv' in self.news_url[url_count - fail_count]:
                    tag = soup.select_one('#CmAdContent')
                    if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                    else:
                        self.news_main_text.append(self.preprocess(tag.text))
                elif 'biz' in soup.select_one('link').get('href'):
                    tag = soup.select_one('body > div > div.middleWrap > div > div.mLeftWrap > div.articleDiv')
                    if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                    else:
                        self.news_main_text.append(self.preprocess(tag.text))
                else:
                    tag = soup.select_one('#atic_txt1')
                    if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                    else:
                        self.news_main_text.append(self.preprocess(tag.text))
            elif '세계일보' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('#article_txt > article')
                if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                else:
                    self.news_main_text.append(self.preprocess(tag.text))
            #elif '조선일보' in self.news_press[url_count - fail_count]:
            #    self.news_main_text.append(self.preprocess(soup.select_one('#articleBody').get_text()))
            elif '중앙일보' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('#article_body')
                if tag == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                else:
                    self.news_main_text.append(self.preprocess(tag.text))
            elif '한겨레' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('div.article-text > div > div.text')
                if tag == None:
                    self.delete(url_count - fail_count)
                    fail_count += 1
                else :
                    self.news_main_text.append(self.preprocess(tag.text))
            elif '한국일보' in self.news_press[url_count - fail_count]:
                tag = soup.select_one('body > div.wrap > div.container.end > div > div > div > p')
                if tag  == None:
                        self.delete(url_count - fail_count)
                        fail_count += 1
                else:
                    text = ""
                    result_set = soup.select('body > div.wrap > div.container.end > div > div > div > p')
                    for p in result_set:
                       text = text + p.text   
                    self.news_main_text.append(self.preprocess(text))

    def save_data(self, date_start, date_end):    # 크롤한 정보들 CSV로 저장
        file = open(date_start + '-' + date_end + '.csv', 'w', encoding='utf-8', newline = '')
        writer = csv.writer(file)
        writer.writerow(["date", "headline", "url", "text"])
        for l in range(0, len(self.news_url)):
            writer.writerow([self.news_date[l], self.news_headline[l], self.news_url[l], self.news_main_text[l]])

        file.close()
    
if __name__ == "__main__":
    date_start = str(input('시작 날짜 입력 ex) 20200905 : '))
    date_end = str(input('끝 날짜 입력 ex) 20200909 : '))

    #print(datetime.datetime.now())
    Crawler = Big_Kinds_Crawler()
    Crawler.crawl_news_url(date_start, date_end)
    Crawler.crawl_news_text()
    Crawler.save_data(date_start, date_end)
    #print(datetime.datetime.now())